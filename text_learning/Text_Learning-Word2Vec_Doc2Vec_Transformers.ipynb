{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eca4c0e-b325-4a6d-909c-e07ba1c1f963",
   "metadata": {},
   "source": [
    "In this notebook, we will move from numeric data to text. In the last year, text has received a lot of attention due to the buzz around ChatGPT. We are going to start a bit simpler by looking at two older approaches for extracting meaning from text: 1) `word2vec` and 2) `doc2vec`. The following code block imports libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0b679-01b1-4fab-b1da-9752951a7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import re\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a97e3-394f-41d4-938b-819fa2ae9659",
   "metadata": {},
   "source": [
    "The following code block reads in the data we will consider, which is a dataset of abstracts for scholarly publications related to human trafficking, labor trafficking, and sex trafficking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731c232-528a-4f24-9173-8c1b0ffc82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = pathlib.Path('abstract_data.json')\n",
    "\n",
    "if data_filepath.exists():\n",
    "    with open(data_filepath) as fin:\n",
    "        data = json.load(fin)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a671eb7-7a7b-405f-9c01-94f45c5b7abc",
   "metadata": {},
   "source": [
    "The following code block prints an example of the data for one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012bced-7171-486b-8584-269e7724d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(data.keys())\n",
    "\n",
    "pprint(data[id_list[75]], width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598a660-20b0-47a5-995e-1276f2d74d8b",
   "metadata": {},
   "source": [
    "The following code block defines a simple function for cleaning the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e2bc2-82b2-4eb7-9d38-2d77518873cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = text.replace('\\n',' ').strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = text.split(' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da83d82-b101-41ce-9daa-d4fbe871ec02",
   "metadata": {},
   "source": [
    "The following code block demonstrates the function. As you can see, it is very rudimentary. If we were doing an analysis for production or a research project, I would invest much more effort in cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e511d50-66cb-41c2-b89d-d680e5e2bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_text(data[id_list[75]]['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee40e-6a60-4f4d-a465-9ada32b3da7c",
   "metadata": {},
   "source": [
    "The following code block applies the function to the data, creating a new `clean_abstract` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2fe6e-0828-4827-9f05-0f55ac3b8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(data, 'Cleaning abstracts'):\n",
    "    data[key]['clean_abstract'] = prepare_text(data[key]['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c7ade-8372-4380-9ff8-6bfd834f065b",
   "metadata": {},
   "source": [
    "The following code block prints an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320865fe-6954-4a0b-8ebb-5fbde8e8e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data[id_list[75]], width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb609d9-fbe6-4ee0-81e5-5c344e8942f7",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "We will first look at Word2Vec. See https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html for additional details. The following code block creates a list of the cleaned abstracts for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44abbe-1302-40c1-bf1a-320d73ad6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [val['clean_abstract'] for val in data.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e2507-3b97-40cc-9e45-c18ecbb6ecb0",
   "metadata": {},
   "source": [
    "The following code block uses `gensim` to fit a `Word2Vec` model using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf5042-a3e9-4883-9fb0-4bd9f13a8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=abstracts,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911d9eb-31e4-44ad-a2ec-9b31edfd16b0",
   "metadata": {},
   "source": [
    "By default, the model uses a *Continuous Bag of Words* training scheme to determine 100-dimensional vector representations for words. A few examples are given in the following code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0becad51-873a-4479-bbff-ebc927b6b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['pimp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd4a1f-2585-480a-b5ea-66bbdb475dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['trafficker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb80312-1407-4e52-878d-fa6b53eb0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['victim']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fae782-8e28-4ce6-ae2d-fca1f8a9ca7a",
   "metadata": {},
   "source": [
    "The following code block defines some test cases we can use to understand what is captured in the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cefaa-0c24-4c17-a318-2d587537f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_test_cases(w2v_model_object):\n",
    "\n",
    "    pimp_trafficker_similarity = cosine_similarity(\n",
    "        w2v_model_object.wv['pimp'].reshape(1, -1), \n",
    "        w2v_model_object.wv['trafficker'].reshape(1, -1)\n",
    "    )[0][0]\n",
    "    print(f' - pimp/trafficker similarity: {pimp_trafficker_similarity:.5f}')\n",
    "\n",
    "    pimp_victim_similarity = cosine_similarity(\n",
    "        w2v_model_object.wv['pimp'].reshape(1, -1), \n",
    "        w2v_model_object.wv['victim'].reshape(1, -1)\n",
    "    )[0][0]\n",
    "    print(f' - pimp/victim similarity: {pimp_victim_similarity:.5f}')\n",
    "\n",
    "    trafficker_most_similar = w2v_model_object.wv.most_similar(\n",
    "        positive=['trafficker'], \n",
    "        topn=5,\n",
    "    )\n",
    "    print('\\n - trafficker (most similar)')\n",
    "    pprint(trafficker_most_similar, indent=4)\n",
    "\n",
    "    victim_most_similar = w2v_model_object.wv.most_similar(\n",
    "        positive=['victim'], \n",
    "        topn=5,\n",
    "    )\n",
    "    print('\\n - victim (most similar)')\n",
    "    pprint(victim_most_similar, indent=4)\n",
    "\n",
    "    internet_most_similar = w2v_model_object.wv.most_similar(\n",
    "        positive=['internet'], \n",
    "        topn=5,\n",
    "    )\n",
    "    print('\\n - internet (most similar)')\n",
    "    pprint(internet_most_similar, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a99bdb-419e-4b62-a6a0-bc981b60b709",
   "metadata": {},
   "source": [
    "The following code block uses the function to examine the outputs for the model we previously fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad23d40-79e9-41ec-818e-f5319e6e8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_cases(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79329f0c-d07f-405d-a6dc-d8b4c6ea3dcb",
   "metadata": {},
   "source": [
    "The following code block fits another model with a larger *window* size and number of *epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f105538-9cc8-498a-988e-ea46af827af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=abstracts, \n",
    "    window=10,\n",
    "    epochs=25,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83c45b1-0105-4add-bd74-4a8ba6ccc1fb",
   "metadata": {},
   "source": [
    "Test results are computed in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b881-b178-44b3-9c6c-462b67ee8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_cases(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760d46c-4d42-4b44-9f74-609d5f1bf975",
   "metadata": {},
   "source": [
    "The following code block fits another model that uses a *skip-gram* training scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5282c3-5b01-404d-9fe9-2c78fbc4585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=abstracts, \n",
    "    window=10,\n",
    "    epochs=25,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa1b17-4b36-4622-8a7d-f4abf8d40243",
   "metadata": {},
   "source": [
    "Test results are computed in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4c349-3e9c-45ea-9f99-e6f4e1d2629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_cases(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495adc9c-559c-47cc-87b6-f5fdff3c7421",
   "metadata": {},
   "source": [
    "#### Doc2Vec\n",
    "\n",
    "We will now look at *Doc2Vec*. Doc2Vec trains a set of *paragraph vectors*, one for each document, in addition to the word vectors. To implement this in `gensim`, we need to use `TaggedDocument` objects. These are defined in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350cb85-bdb7-4719-a477-1a865f202f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(val['clean_abstract'], [key]) for key, val in data.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edbb1f-1e59-4820-976c-99f01a42711f",
   "metadata": {},
   "source": [
    "The following code block trains a basic `Doc2Vec` model using the *gensim* defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c6efe-9170-4a5e-a999-d09b10607bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(\n",
    "    documents=documents,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72141493-954c-4e9f-9e48-0fe3d24a2ba0",
   "metadata": {},
   "source": [
    "Since Doc2Vec still computes word vectors, we can still perform our tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8e440-cef4-4af5-83a7-0c1b4cf20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_cases(d2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f934cd5-b197-4628-950d-54ac0caa8a1d",
   "metadata": {},
   "source": [
    "We can get the *paragraph vector* for a document by indexing with the ID we used when defining the `TaggedDocument` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397335b-a35e-4290-a06d-42f275065719",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.dv[id_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c04aa2-8a64-4b73-8e82-074636fbccd7",
   "metadata": {},
   "source": [
    "We can use the paragraph vectors to identify similar pieces of text. To demonstrate, here are the IDS for two papers I have been involved with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550eb9b-c390-42f7-81e4-aabef8d247ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ids = [\n",
    "    'SCOPUS_ID:85147005648', \n",
    "    'SCOPUS_ID:85097137525',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f87ce-3a4d-45c4-a91d-96306b58b481",
   "metadata": {},
   "source": [
    "As you might expect, the Doc2Vec model finds them to be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a20dcb-18fd-4dd6-8fd8-faff45e53f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(\n",
    "    d2v_model.dv[my_ids[0]].reshape(1, -1),\n",
    "    d2v_model.dv[my_ids[1]].reshape(1, -1),\n",
    ")[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014cf81-0fc7-459d-9d90-5c251ca93d61",
   "metadata": {},
   "source": [
    "We can use the `most_similar` method to easily get back a list of the IDs for the most similar documents along with the similarity score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584009b-ef16-413d-998a-5bee54098079",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.dv.most_similar(d2v_model.dv[my_ids[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4eb9a-3cda-4133-9bf1-51a1edd2764d",
   "metadata": {},
   "source": [
    "The following code block prints the titles for the most similar 25 documents for both of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230ff10-bd92-44ce-a0f7-7ff4002b1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in my_ids:\n",
    "    target_paper = data[target]['title']\n",
    "    abstract = data[target]['abstract']\n",
    "    print(f\"Target: {target_paper}\\n\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    print('\\n')\n",
    "    for sid, similarity in d2v_model.dv.most_similar(d2v_model.dv[target], topn=25):\n",
    "        print(f\" - {data[sid]['title']} ({similarity: .3f})\")\n",
    "    print('*'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4ccf9-b3e3-4fb7-b50b-f31d89c4b206",
   "metadata": {},
   "source": [
    "#### Transformers\n",
    "\n",
    "We will now look at using transformer models that are publicly available via HuggingFace (https://huggingface.co/). We will use PyTorch as the neural network framework. In class, we will look at how to install PyTorch for CUDA. The following code block imports PyTourch and checks to see if CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a54c5-fbe2-4bee-aa12-430b4435da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2890e-7a86-4b65-be46-c1935582810c",
   "metadata": {},
   "source": [
    "The following code block checks the number of CUDA devices available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85107225-f881-405f-84d5-897527a98c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06901d25-af79-4d4d-9724-c1f60a1d15e2",
   "metadata": {},
   "source": [
    "We will generate the embeddings for abstracts using the `SentenceTransformer` package. The following code block imports the library and specifes that we will use the `all-MiniLM-L6-v2` model (see https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1edd5-570f-4e09-ab07-caafe156f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f189db-ad9c-4f64-bae3-3c64e41e05e4",
   "metadata": {},
   "source": [
    "The `SentenceTransformer` will handle tokenization, so we only need a list of the raw abstract texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb781095-c0aa-4669-bcde-610b76413c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_strings = [val['abstract'] for val in data.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefab2a-afb9-4812-9b13-4e46d2927054",
   "metadata": {},
   "source": [
    "The following code block times the generation of embeddings on 500 of the abstracts when using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cf6cd-c294-49b7-bd76-d76fd50b8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = st_model.encode(abstract_strings[:500], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ae88c-25d1-45c3-b02d-dd3595ee5cd1",
   "metadata": {},
   "source": [
    "The following code block times the generation of embeddings on 500 of the abstracts when using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f17f3d-d44e-4056-95e9-95534b24a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = st_model.encode(abstract_strings[:500], device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e656fc-bbc5-4fba-8a77-c7e15731278b",
   "metadata": {},
   "source": [
    "The following code block generates the embeddings for all abstracts using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c017259-8017-40a6-a972-4dc73fdd4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = st_model.encode(abstract_strings, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d3d78-8d45-4ddf-874a-c26417f30089",
   "metadata": {},
   "source": [
    "The following code block creates a `DataFrame` of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb8e3f-8485-44c2-a8a6-74b06936f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(\n",
    "    embeddings,\n",
    "    index=list(data.keys()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413611f1-ad32-4579-a239-8332ed98fdef",
   "metadata": {},
   "source": [
    "The following code block prints the titles for the most similar 25 documents for both of the targets defined in `my_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f474db8-b783-453f-82e2-c6705781bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in my_ids:\n",
    "\n",
    "    similarities = cosine_similarity(\n",
    "        embeddings_df.loc[target].values.reshape(1, -1),\n",
    "        embeddings_df.values,\n",
    "    )\n",
    "    \n",
    "    similar_articles = pd.Series(\n",
    "        similarities.flatten(),\n",
    "        index=list(data.keys()),\n",
    "    ).nlargest(25).to_dict()\n",
    "    \n",
    "    target_paper = data[target]['title']\n",
    "    abstract = data[target]['abstract']\n",
    "    print(f\"Target: {target_paper}\\n\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    print('\\n')\n",
    "    for sid, similarity in similar_articles.items():\n",
    "        print(f\" - {data[sid]['title']} ({similarity: .3f})\")\n",
    "    print('*'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712241da-7ce4-4d5e-91a0-11f43b9181eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
